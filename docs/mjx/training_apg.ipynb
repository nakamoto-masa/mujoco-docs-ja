{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpkYHwCqk7W-"
   },
   "source": [
    "![MuJoCo banner](https://raw.githubusercontent.com/google-deepmind/mujoco/main/banner.png)\n",
    "\n",
    "# <h1><center>チュートリアル  <a href=\"https://colab.research.google.com/github/google-deepmind/mujoco/blob/main/mjx/training_apg.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" width=\"140\" align=\"center\"/></a></center></h1>\n",
    "\n",
    "このノートブックは、MuJoCoのJAXベース実装である [**MuJoCo XLA (MJX)**](https://github.com/google-deepmind/mujoco/blob/main/mjx) における微分可能物理を用いたポリシー学習のチュートリアルを提供します。\n",
    "\n",
    "**GPUアクセラレーション付きのColabランタイムが必要です。** CPU専用ランタイムを使用している場合は、メニュー「ランタイム > ランタイムのタイプを変更」から切り替えることができます。\n",
    "\n",
    "\n",
    "このノートブックは [Jing Yuan Luo](https://github.com/Andrew-Luo1) によって作成されました。\n",
    "\n",
    "<!-- Copyright 2021 DeepMind Technologies Limited\n",
    "\n",
    "     Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "     you may not use this file except in compliance with the License.\n",
    "     You may obtain a copy of the License at\n",
    "\n",
    "         http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "     Unless required by applicable law or agreed to in writing, software\n",
    "     distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "     See the License for the specific language governing permissions and\n",
    "     limitations under the License.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ポリシー学習とポリシー勾配\n",
    "\n",
    "ここでは、ポリシー学習の概要を振り返り、MJXの微分可能性をポリシー学習にどのように活用できるかを文脈に沿って説明します。以下の概念に馴染みがない場合は、多くの優れたリソースが [オンライン](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html) で利用できます！\n",
    "\n",
    "ポリシー学習の目標は、ある期間にわたる総報酬 $\\sum r_t$ を最大化するアクション $a_t \\sim \\pi(\\cdot| x_t, \\theta)$ を出力する制御ポリシー $\\pi$ を見つけることです。ここで $r_t$ は時刻tの状態とアクションで評価される報酬関数の略記です：\n",
    "\n",
    "$$r_t = r(x_t, a_t)$$\n",
    "\n",
    "$\\theta$ はポリシーのパラメータであり、ポリシーがニューラルネットワークの場合は重みに相当します。**ポリシー勾配法** は、重みに関する報酬の勾配を推定し、この値を勾配降下法や [Adam](https://arxiv.org/abs/1412.6980) などの一次最適化アルゴリズムで使用します。ポリシー勾配の推定方法は、想定する状態遷移モデルによって異なります。\n",
    "\n",
    "#### ゼロ次ポリシー勾配（ZoPG）\n",
    "\n",
    "`mjx.step` をシミュレーション関数fとして参照し、fの値のみに依存するゼロ次勾配と、そのヤコビアンに依存する一次勾配を区別するために、いくつかの [用語](https://arxiv.org/abs/2202.00817) を借用します。\n",
    "\n",
    "標準的な [PPO](https://github.com/google/brax/blob/main/brax/training/agents/ppo/train.py) などの強化学習（RL）アルゴリズムは、確率的状態遷移モデル $x_{t+1} \\sim P(\\cdot | x_t, a_t)$ を仮定します。これにより、以下の形式のZoPGが得られます：\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\pi_\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[ \\sum \\nabla_\\theta \\log\\pi_\\theta (a_t | s_t) R(\\tau) \\right]\n",
    "$$\n",
    "\n",
    "ここで $R(\\tau)$ はロールアウト $\\tau = \\{x_t, a_t\\}_{t=0}^{T}$ に依存する何らかの関数です。この手法の人気と精緻化に向けた広範な研究にもかかわらず、勾配の分散が高いという基本的な特性があります。これにより、オプティマイザはポリシー空間を徹底的に探索でき、堅牢でしばしば驚くほど優れたポリシーを実現できます。しかし、分散が高いため、収束には多くのサンプル $(x_t, a_t)$ が必要になります。\n",
    "\n",
    "#### 一次ポリシー勾配（FoPG）\n",
    "一方、決定論的状態遷移モデル $x_{t+1} = f(x_t, a_t)$ を仮定すると、一次ポリシー勾配が得られます。他の一般的な名称には、解析的ポリシー勾配（APG）や時間方向逆伝播（BPTT）があります。状態の進化を確率的なブラックボックスとしてモデル化するZoPG法とは異なり、FoPGはシミュレーション関数fのヤコビアンを明示的に含みます。例として、報酬が状態のみに依存する場合の報酬 $r_t$ の勾配を見てみましょう。\n",
    "$$\n",
    "\\frac{\\partial r_t}{\\partial \\theta} = \\frac{\\partial r_t}{\\partial x_t}\\frac{\\partial x_t}{\\partial \\theta} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial x_t}{\\partial \\theta} = \\color{blue}{\\frac{\\partial f(x_{t-1}, a_{t-1})}{\\partial x_{t-1}}}\\frac{\\partial x_{t-1}}{\\partial \\theta} + \\color{blue}{\\frac{\\partial f(x_{t-1}, a_{t-1})}{\\partial a_{t-1}}} \\frac{\\partial a_{t-1}}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "上式の青色の項はMJXの微分可能性によって可能となるものであり、FoPGとZoPGの主要な違いです。重要な考慮点は、接触点付近でこれらのヤコビアンがどのように見えるかです。ヤコビアン内の特定の勾配が病的になりうる理由を理解するために、硬い球が大理石のブロックに向かって落下する状況を想像してください。地面に触れる直前に、速度は距離に対してどのように変化するでしょうか ($\\frac{\\partial \\dot{z}_t}{\\partial z_t}$) ？これは [ハードコンタクト](https://arxiv.org/html/2404.02887v1) による **情報量のない勾配** のケースです。幸い、MuJoCoのデフォルトの接触設定はFoPGによる学習に十分な [ソフト](https://mujoco.readthedocs.io/en/stable/computation/index.html#soft-contact-model) さを持っています。ソフトコンタクトでは、剛体接触のように即座に偏向するのに十分な力を提供するのではなく、ボールが地面に侵入するにつれて増加する力を加えます。\n",
    "\n",
    "FoPGについて考える有用な方法は、連鎖律と計算グラフを通じたものです。以下は、報酬がアクションに依存しない場合に $r_2$ がポリシー勾配にどのように影響するかを示しています：\n",
    "\n",
    "<img src=\"../doc/images/mjx/apg_diagram.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "この例では3つの異なる勾配の連鎖があることに注目してください。赤い経路は、直前のアクションが状態にどのように影響したかを考慮します。青い経路は *時間方向逆伝播* という名前の由来を説明し、アクションが下流の報酬にどのように影響するかを捉えます。最も直感的でないのは緑の連鎖かもしれません。これは、報酬がアクションの前のアクションへの依存性にどのように依存するかを示しています。経験的に、 `jax.lax.stop_grad` によってこれら3つの経路の *いずれか* をブロックすると、ポリシー学習が著しく妨げられることがわかっています。 $x_t$ のバックボーンの長さが増すにつれて、 [勾配爆発](https://arxiv.org/abs/2111.05803) が重要な考慮事項になります。実際には、下流の勾配を減衰させるか、定期的に勾配を切り詰めることで解決できます。\n",
    "\n",
    "**FoPGの注意点**\n",
    "\n",
    "FoPGは特に [状態空間の次元が増加する](https://arxiv.org/abs/2204.07137) につれて非常にサンプル効率が高いことが示されていますが、根本的な欠点の一つとして、勾配の分散が低いため、FoPGはZoPGよりも探索能力が劣り、問題の定式化においてより明示的であることが求められます。\n",
    "\n",
    "さらに、ロボットが倒れた時の大きなペナルティなど、不連続な報酬の定式化はRLにおいて至る所に存在します。FoPGはそのようなペナルティを通じて逆伝播できないため、FoPGでロバストなポリシーを設計することは [かなり困難](https://arxiv.org/abs/2403.14864) になる可能性があります。\n",
    "\n",
    "最後に、サンプル効率にもかかわらず、FoPG法はウォールクロック時間で苦戦する可能性があります。勾配の分散が低いため、 [RL](https://arxiv.org/abs/2109.11978) とは異なり、データ収集の大規模並列化による恩恵が大きくありません。さらに、ポリシー勾配は通常、自動微分を介して計算されます。これはシミュレーションの順方向展開と比べて3〜5倍遅く、メモリを大量に消費します。メモリ要件は $O(m \\cdot (m+n) \\cdot T)$ でスケーリングされます。ここで、mとnは状態と制御の次元、$m \\cdot (m+n)$ はヤコビアンの次元、Tは伝播されるステップ数です。\n",
    "\n",
    "特定のモデルでは、 `mjx.step` を通じた自動微分が現在 [nan勾配](https://github.com/google-deepmind/mujoco/issues/1517) を引き起こすことに注意してください。現在のところ、メモリ要件と学習時間が倍増するコストを払って、倍精度浮動小数点数を使用することでこの問題に対処しています。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**論文**\n",
    "\n",
    "学術的な文脈でこの研究を使用する場合は、以下の論文を引用してください：\n",
    "\n",
    "```\n",
    "@misc{luo2024residual,\n",
    "  title={Residual Policy Learning for Perceptive Quadruped Control Using Differentiable Simulation},\n",
    "  author={Luo, Jing Yuan and Song, Yunlong and Klemm, Victor and Shi, Fan and Scaramuzza, Davide and Hutter, Marco},\n",
    "  year={2024},\n",
    "  eprint={2410.03076},\n",
    "  archivePrefix={arXiv},\n",
    "  primaryClass={cs.RO},\n",
    "  url={https://doi.org/10.48550/arXiv.2410.03076}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**このチュートリアルの内容**\n",
    "\n",
    "このチュートリアルでは、Braxのシンプルな APG [アルゴリズム](https://github.com/google/brax/tree/main/brax/training/agents/apg) を使用して、FoPGの2つの使用方法を示します。このアルゴリズムは基本的に、FoPGを使ってポリシーに対するライブ勾配降下を行い、短い時間窓でポリシーを展開し、そのデータを使ってポリシー更新を行い、その後中断した箇所から継続します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セットアップ：インポートとインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MuJoCo、MJX、Braxのインストール\n",
    "!pip install mujoco\n",
    "!pip install mujoco_mjx\n",
    "!pip install brax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title MuJoCoのインストールが成功したか確認\n",
    "\n",
    "# GPUレンダリングのセットアップ\n",
    "from google.colab import files\n",
    "import distutils.util\n",
    "import os\n",
    "import subprocess\n",
    "if subprocess.run('nvidia-smi').returncode:\n",
    "  raise RuntimeError(\n",
    "      'Cannot communicate with GPU. '\n",
    "      'Make sure you are using a GPU Colab runtime. '\n",
    "      'Go to the Runtime menu and select Choose runtime type.')\n",
    "\n",
    "# glvndがNvidia EGLドライバを検出できるようにICDコンフィグを追加します。\n",
    "# これは通常Nvidiaドライバパッケージの一部としてインストールされますが、Colabの\n",
    "# カーネルはAPT経由でドライバをインストールしないため、ICDが欠けています。\n",
    "# (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n",
    "NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
    "if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
    "  with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
    "    f.write(\"\"\"{\n",
    "    \"file_format_version\" : \"1.0.0\",\n",
    "    \"ICD\" : {\n",
    "        \"library_path\" : \"libEGL_nvidia.so.0\"\n",
    "    }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# MuJoCoがEGLレンダリングバックエンドを使用するように設定（GPU必須）\n",
    "print('Setting environment variable to use GPU rendering:')\n",
    "%env MUJOCO_GL=egl\n",
    "\n",
    "# インストールが成功したか確認\n",
    "try:\n",
    "  print('Checking that the installation succeeded:')\n",
    "  import mujoco\n",
    "  mujoco.MjModel.from_xml_string('<mujoco/>')\n",
    "except Exception as e:\n",
    "  raise e from RuntimeError(\n",
    "      'Something went wrong during installation. Check the shell output above '\n",
    "      'for more information.\\n'\n",
    "      'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
    "      'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
    "\n",
    "print('Installation successful.')\n",
    "\n",
    "# その他のインポートとヘルパー関数\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "# グラフィックスとプロット\n",
    "print('Installing mediapy:')\n",
    "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
    "!pip install -q mediapy\n",
    "import mediapy as media\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# numpyの出力をより読みやすく\n",
    "np.set_printoptions(precision=3, suppress=True, linewidth=100)\n",
    "\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title MuJoCo、MJX、Braxのインポート\n",
    "\n",
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.8\" # 0.9だとラグが大きすぎる\n",
    "from datetime import datetime\n",
    "import functools\n",
    "\n",
    "# 数学\n",
    "import jax.numpy as jp\n",
    "import numpy as np\n",
    "import jax\n",
    "from jax import config # 解析的勾配は倍精度でより良く動作する\n",
    "config.update(\"jax_debug_nans\", True)\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "config.update('jax_default_matmul_precision', jax.lax.Precision.HIGH)\n",
    "from brax import math\n",
    "\n",
    "# シミュレーション\n",
    "import mujoco\n",
    "import mujoco.mjx as mjx\n",
    "\n",
    "# Brax\n",
    "from brax import envs\n",
    "from brax.base import Motion, Transform\n",
    "from brax.io import mjcf\n",
    "from brax.envs.base import PipelineEnv, State\n",
    "from brax.mjx.pipeline import _reformat_contact\n",
    "from brax.training.acme import running_statistics\n",
    "from brax.io import model\n",
    "\n",
    "# アルゴリズム\n",
    "from brax.training.agents.apg import train as apg\n",
    "from brax.training.agents.apg import networks as apg_networks\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "\n",
    "# サポート\n",
    "from etils import epath\n",
    "import mediapy as media\n",
    "import matplotlib.pyplot as plt\n",
    "from ml_collections import config_dict\n",
    "from typing import Any, Dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 四足歩行ロボット環境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/google-deepmind/mujoco_menagerie.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_path = epath.Path('mujoco_menagerie/anybotics_anymal_c/scene_mjx.xml').as_posix()\n",
    "\n",
    "mj_model = mujoco.MjModel.from_xml_path(xml_path)\n",
    "\n",
    "if 'renderer' not in dir():\n",
    "    renderer = mujoco.Renderer(mj_model)\n",
    "\n",
    "init_q = mj_model.keyframe('standing').qpos\n",
    "\n",
    "mj_data = mujoco.MjData(mj_model)\n",
    "mj_data.qpos = init_q\n",
    "mujoco.mj_forward(mj_model, mj_data)\n",
    "\n",
    "renderer.update_scene(mj_data)\n",
    "media.show_image(renderer.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ロールアウトのレンダリング\n",
    "def render_rollout(reset_fn, step_fn, \n",
    "                   inference_fn, env, \n",
    "                   n_steps = 200, camera=None,\n",
    "                   seed=0):\n",
    "    rng = jax.random.key(seed)\n",
    "    render_every = 3\n",
    "    state = reset_fn(rng)\n",
    "    rollout = [state.pipeline_state]\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        act_rng, rng = jax.random.split(rng)\n",
    "        ctrl, _ = inference_fn(state.obs, act_rng)\n",
    "        state = step_fn(state, ctrl)\n",
    "        if i % render_every == 0:\n",
    "            rollout.append(state.pipeline_state)\n",
    "\n",
    "    media.show_video(env.render(rollout, camera=camera), \n",
    "                     fps=1.0 / (env.dt*render_every),\n",
    "                     codec='gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## スタディ1：キネマティクスの模倣\n",
    "\n",
    "FoPGは [模倣学習](https://openreview.net/forum?id=06mk-epSwZ) においてうまく機能することが示されています。特に、エージェントの状態が参照状態から離れすぎた場合に参照状態にリセットされる設定で効果的です。このセクションでは、その場でのトロット歩行を学習します。PDコントローラのゲインが限られているため、高速なトロット歩行は決して自明ではありません！\n",
    "\n",
    "RL環境には3つの報酬があります：\n",
    "- min_reference_tracking は最小座標での参照モーションからの誤差にペナルティを与えます。これにより、ポリシーの出力がより正確になります。\n",
    "- reference_tracking は最大座標での誤差にペナルティを与え、学習の安定性を向上させます。\n",
    "- feet_height は追跡するボディの位置と速度のバランスを調整し、足の *位置* に追加のインセンティブを与えます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参照キネマティクスの設計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_wave(t, step_period, scale):\n",
    "    _cos_wave = -jp.cos(((2*jp.pi)/step_period)*t)\n",
    "    return _cos_wave * (scale/2) + (scale/2)\n",
    "\n",
    "def dcos_wave(t, step_period, scale):\n",
    "    \"\"\" \n",
    "    cos波の導関数（参照速度用）\n",
    "    \"\"\"\n",
    "    return ((scale*jp.pi) / step_period) * jp.sin(((2*jp.pi)/step_period)*t)\n",
    "\n",
    "def make_kinematic_ref(sinusoid, step_k, scale=0.3, dt=1/50):\n",
    "    \"\"\" \n",
    "    12個の脚関節のトロット歩行キネマティクスを作成します。\n",
    "    step_kは足を上げ下げするのにかかるタイムステップ数です。\n",
    "    歩行サイクルは 2 * step_k * dt 秒の長さです。\n",
    "    \"\"\"\n",
    "    \n",
    "    _steps = jp.arange(step_k)\n",
    "    step_period = step_k * dt\n",
    "    t = _steps * dt\n",
    "    \n",
    "    wave = sinusoid(t, step_period, scale)\n",
    "    # アクティブな前脚の1ステップ分のコマンド\n",
    "    fleg_cmd_block = jp.concatenate(\n",
    "        [jp.zeros((step_k, 1)),\n",
    "        wave.reshape(step_k, 1),\n",
    "        -2*wave.reshape(step_k, 1)],\n",
    "        axis=1\n",
    "    )\n",
    "    # 立ち姿勢の設定では前脚と後脚が反転\n",
    "    h_leg_cmd_bloc = -1 * fleg_cmd_block\n",
    "\n",
    "    block1 = jp.concatenate([\n",
    "        jp.zeros((step_k, 3)),\n",
    "        fleg_cmd_block,\n",
    "        h_leg_cmd_bloc,\n",
    "        jp.zeros((step_k, 3))],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    block2 = jp.concatenate([\n",
    "        fleg_cmd_block,\n",
    "        jp.zeros((step_k, 3)),\n",
    "        jp.zeros((step_k, 3)),\n",
    "        h_leg_cmd_bloc],\n",
    "        axis=1\n",
    "    )\n",
    "    # 1ステップサイクルで、両方のアクティブな脚ペアが非アクティブとアクティブフェーズを持つ\n",
    "    step_cycle = jp.concatenate([block1, block2], axis=0)\n",
    "    return step_cycle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses  = make_kinematic_ref(cos_wave, step_k=25)\n",
    "\n",
    "frames = []\n",
    "init_q = mj_model.keyframe('standing').qpos\n",
    "mj_data.qpos = init_q\n",
    "default_ap = init_q[7:]\n",
    "\n",
    "for i in range(len(poses)):\n",
    "    mj_data.qpos[7:] = poses[i] + default_ap\n",
    "    mujoco.mj_forward(mj_model, mj_data)\n",
    "    renderer.update_scene(mj_data)\n",
    "    frames.append(renderer.render())\n",
    "\n",
    "media.show_video(frames, fps=50, codec='gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RL環境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "  def get_default_rewards_config():\n",
    "    default_config = config_dict.ConfigDict(\n",
    "        dict(\n",
    "            scales=config_dict.ConfigDict(\n",
    "              dict(\n",
    "                min_reference_tracking = -2.5 * 3e-3, # 大きさを均等にするため\n",
    "                reference_tracking = -1.0,\n",
    "                feet_height = -1.0\n",
    "                )\n",
    "              )\n",
    "            )\n",
    "    )\n",
    "    return default_config\n",
    "\n",
    "  default_config = config_dict.ConfigDict(\n",
    "      dict(rewards=get_default_rewards_config(),))\n",
    "\n",
    "  return default_config\n",
    "\n",
    "# 数学関数（https://github.com/jiawei-ren/diffmimic より）\n",
    "def quaternion_to_matrix(quaternions):\n",
    "    r, i, j, k = quaternions[..., 0], quaternions[..., 1], quaternions[..., 2], quaternions[..., 3]\n",
    "    two_s = 2.0 / (quaternions * quaternions).sum(-1)\n",
    "\n",
    "    o = jp.stack(\n",
    "        (\n",
    "            1 - two_s * (j * j + k * k),\n",
    "            two_s * (i * j - k * r),\n",
    "            two_s * (i * k + j * r),\n",
    "            two_s * (i * j + k * r),\n",
    "            1 - two_s * (i * i + k * k),\n",
    "            two_s * (j * k - i * r),\n",
    "            two_s * (i * k - j * r),\n",
    "            two_s * (j * k + i * r),\n",
    "            1 - two_s * (i * i + j * j),\n",
    "        ),\n",
    "        -1,\n",
    "    )\n",
    "    return o.reshape(quaternions.shape[:-1] + (3, 3))\n",
    "\n",
    "def matrix_to_rotation_6d(matrix):\n",
    "    batch_dim = matrix.shape[:-2]\n",
    "    return matrix[..., :2, :].reshape(batch_dim + (6,))\n",
    "\n",
    "def quaternion_to_rotation_6d(quaternion):\n",
    "    return matrix_to_rotation_6d(quaternion_to_matrix(quaternion))\n",
    "\n",
    "class TrotAnymal(PipelineEnv):\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      termination_height: float=0.25,\n",
    "      **kwargs,\n",
    "  ):\n",
    "    step_k = kwargs.pop('step_k', 25)\n",
    "\n",
    "    physics_steps_per_control_step = 10\n",
    "    kwargs['n_frames'] = kwargs.get(\n",
    "        'n_frames', physics_steps_per_control_step)\n",
    "\n",
    "    mj_model = mujoco.MjModel.from_xml_path(xml_path)\n",
    "    kp = 230\n",
    "    mj_model.actuator_gainprm[:, 0] = kp\n",
    "    mj_model.actuator_biasprm[:, 1] = -kp\n",
    "\n",
    "    sys = mjcf.load_model(mj_model)\n",
    "\n",
    "    super().__init__(sys=sys, **kwargs)    \n",
    "    \n",
    "    self.termination_height = termination_height\n",
    "    \n",
    "    self._init_q = mj_model.keyframe('standing').qpos\n",
    "    \n",
    "    self.err_threshold = 0.4 # diffmimic; 論文の値\n",
    "    \n",
    "    self._default_ap_pose = mj_model.keyframe('standing').qpos[7:]\n",
    "    self.reward_config = get_config()\n",
    "\n",
    "    self.action_loc = self._default_ap_pose\n",
    "    self.action_scale = jp.array([0.2, 0.8, 0.8] * 4)\n",
    "    \n",
    "    self.feet_inds = jp.array([21,28,35,42]) # LF, RF, LH, RH\n",
    "\n",
    "    #### 模倣参照\n",
    "    kinematic_ref_qpos = make_kinematic_ref(\n",
    "      cos_wave, step_k, scale=0.3, dt=self.dt)\n",
    "    kinematic_ref_qvel = make_kinematic_ref(\n",
    "      dcos_wave, step_k, scale=0.3, dt=self.dt)\n",
    "    \n",
    "    self.l_cycle = jp.array(kinematic_ref_qpos.shape[0])\n",
    "    \n",
    "    # 状態空間全体に拡張\n",
    "\n",
    "    kinematic_ref_qpos += self._default_ap_pose\n",
    "    ref_qs = np.tile(self._init_q.reshape(1, 19), (self.l_cycle, 1))\n",
    "    ref_qs[:, 7:] = kinematic_ref_qpos\n",
    "    self.kinematic_ref_qpos = jp.array(ref_qs)\n",
    "    \n",
    "    ref_qvels = np.zeros((self.l_cycle, 18))\n",
    "    ref_qvels[:, 6:] = kinematic_ref_qvel\n",
    "    self.kinematic_ref_qvel = jp.array(ref_qvels)\n",
    "\n",
    "    # JIT時間と学習のウォールクロック時間を大幅に削減できる\n",
    "    self.pipeline_step = jax.checkpoint(self.pipeline_step, \n",
    "      policy=jax.checkpoint_policies.dots_with_no_batch_dims_saveable)\n",
    "    \n",
    "  def reset(self, rng: jax.Array) -> State:\n",
    "    # 決定論的初期化\n",
    "\n",
    "    qpos = jp.array(self._init_q)\n",
    "    qvel = jp.zeros(18)\n",
    "    \n",
    "    data = self.pipeline_init(qpos, qvel)\n",
    "\n",
    "    # 地面上に配置\n",
    "    pen = jp.min(data.contact.dist)\n",
    "    qpos = qpos.at[2].set(qpos[2] - pen)\n",
    "    data = self.pipeline_init(qpos, qvel)\n",
    "\n",
    "    state_info = {\n",
    "        'rng': rng,\n",
    "        'steps': 0.0,\n",
    "        'reward_tuple': {\n",
    "            'reference_tracking': 0.0,\n",
    "            'min_reference_tracking': 0.0,\n",
    "            'feet_height': 0.0\n",
    "        },\n",
    "        'last_action': jp.zeros(12), # MJXチュートリアルより\n",
    "        'kinematic_ref': jp.zeros(19),\n",
    "    }\n",
    "\n",
    "    x, xd = data.x, data.xd\n",
    "    obs = self._get_obs(data.qpos, x, xd, state_info)\n",
    "    reward, done = jp.zeros(2)\n",
    "    metrics = {}\n",
    "    for k in state_info['reward_tuple']:\n",
    "      metrics[k] = state_info['reward_tuple'][k]\n",
    "    state = State(data, obs, reward, done, metrics, state_info)\n",
    "    return jax.lax.stop_gradient(state)\n",
    "  \n",
    "  def step(self, state: State, action: jax.Array) -> State:\n",
    "    action = jp.clip(action, -1, 1) # 生のアクション\n",
    "\n",
    "    action = self.action_loc + (action * self.action_scale)\n",
    "\n",
    "    data = self.pipeline_step(state.pipeline_state, action)\n",
    "    \n",
    "    ref_qpos = self.kinematic_ref_qpos[jp.array(state.info['steps']%self.l_cycle, int)]\n",
    "    ref_qvel = self.kinematic_ref_qvel[jp.array(state.info['steps']%self.l_cycle, int)]\n",
    "    \n",
    "    # 最大座標の計算\n",
    "    ref_data = data.replace(qpos=ref_qpos, qvel=ref_qvel)\n",
    "    ref_data = mjx.forward(self.sys, ref_data)\n",
    "    ref_x, ref_xd = ref_data.x, ref_data.xd\n",
    "\n",
    "    state.info['kinematic_ref'] = ref_qpos\n",
    "\n",
    "    # 観測データ\n",
    "    x, xd = data.x, data.xd\n",
    "    obs = self._get_obs(data.qpos, x, xd, state.info)\n",
    "\n",
    "    # 転倒または落下した場合に終了\n",
    "    done = 0.0\n",
    "    done = jp.where(x.pos[0, 2] < self.termination_height, 1.0, done)\n",
    "    up = jp.array([0.0, 0.0, 1.0])\n",
    "    done = jp.where(jp.dot(math.rotate(up, x.rot[0]), up) < 0, 1.0, done)\n",
    "\n",
    "    # 報酬\n",
    "    reward_tuple = {\n",
    "        'reference_tracking': (\n",
    "          self._reward_reference_tracking(x, xd, ref_x, ref_xd)\n",
    "          * self.reward_config.rewards.scales.reference_tracking\n",
    "        ),\n",
    "        'min_reference_tracking': (\n",
    "          self._reward_min_reference_tracking(ref_qpos, ref_qvel, state)\n",
    "          * self.reward_config.rewards.scales.min_reference_tracking\n",
    "        ),\n",
    "        'feet_height': (\n",
    "          self._reward_feet_height(data.geom_xpos[self.feet_inds][:, 2]\n",
    "                                   ,ref_data.geom_xpos[self.feet_inds][:, 2])\n",
    "          * self.reward_config.rewards.scales.feet_height\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    reward = sum(reward_tuple.values())\n",
    "\n",
    "    # 状態管理\n",
    "    state.info['reward_tuple'] = reward_tuple\n",
    "    state.info['last_action'] = action # 観測に使用\n",
    "\n",
    "    for k in state.info['reward_tuple'].keys():\n",
    "      state.metrics[k] = state.info['reward_tuple'][k]\n",
    "\n",
    "    state = state.replace(\n",
    "        pipeline_state=data, obs=obs, reward=reward,\n",
    "        done=done)\n",
    "    \n",
    "    #### 参照から離れすぎた場合に状態を参照にリセット\n",
    "    error = (((x.pos - ref_x.pos) ** 2).sum(-1)**0.5).mean()\n",
    "    to_reference = jp.where(error > self.err_threshold, 1.0, 0.0)\n",
    "\n",
    "    to_reference = jp.array(to_reference, dtype=int) # 出力の型を入力と同じに保つ\n",
    "    ref_data = self.mjx_to_brax(ref_data)\n",
    "\n",
    "    data = jax.tree_util.tree_map(lambda x, y: \n",
    "                                  jp.array((1-to_reference)*x + to_reference*y, x.dtype), data, ref_data)\n",
    "    \n",
    "    x, xd = data.x, data.xd # データが変更された可能性がある\n",
    "    obs = self._get_obs(data.qpos, x, xd, state.info)\n",
    "    \n",
    "    return state.replace(pipeline_state=data, obs=obs)\n",
    "    \n",
    "  def _get_obs(self, qpos: jax.Array, x: Transform, xd: Motion,\n",
    "               state_info: Dict[str, Any]) -> jax.Array:\n",
    "\n",
    "    inv_base_orientation = math.quat_inv(x.rot[0])\n",
    "    local_rpyrate = math.rotate(xd.ang[0], inv_base_orientation)\n",
    "\n",
    "    obs_list = []\n",
    "    # ヨーレート\n",
    "    obs_list.append(jp.array([local_rpyrate[2]]) * 0.25)\n",
    "    # 射影重力\n",
    "    obs_list.append(\n",
    "        math.rotate(jp.array([0.0, 0.0, -1.0]), inv_base_orientation))\n",
    "    # モーター角度\n",
    "    angles = qpos[7:19]\n",
    "    obs_list.append(angles - self._default_ap_pose)\n",
    "    # 前回のアクション\n",
    "    obs_list.append(state_info['last_action'])\n",
    "    # キネマティクス参照\n",
    "    kin_ref = self.kinematic_ref_qpos[jp.array(state_info['steps']%self.l_cycle, int)]\n",
    "    obs_list.append(kin_ref[7:]) # 最初の7インデックスは固定\n",
    "\n",
    "    obs = jp.clip(jp.concatenate(obs_list), -100.0, 100.0)\n",
    "\n",
    "    return obs\n",
    "  \n",
    "  def mjx_to_brax(self, data):\n",
    "    \"\"\" \n",
    "    コアMJXデータ構造にBraxラッパーを適用します。\n",
    "    \"\"\"\n",
    "    q, qd = data.qpos, data.qvel\n",
    "    x = Transform(pos=data.xpos[1:], rot=data.xquat[1:])\n",
    "    cvel = Motion(vel=data.cvel[1:, 3:], ang=data.cvel[1:, :3])\n",
    "    offset = data.xpos[1:, :] - data.subtree_com[self.sys.body_rootid[1:]]\n",
    "    offset = Transform.create(pos=offset)\n",
    "    xd = offset.vmap().do(cvel)\n",
    "    data = _reformat_contact(self.sys, data)\n",
    "    return data.replace(q=q, qd=qd, x=x, xd=xd)\n",
    "\n",
    "\n",
    "  # ------------ 報酬関数 ----------------\n",
    "  def _reward_reference_tracking(self, x, xd, ref_x, ref_xd):\n",
    "    \"\"\"\n",
    "    慣性フレームのボディ位置に基づく報酬。\n",
    "    特に、姿勢の高次元表現を使用します。\n",
    "    \"\"\"\n",
    "\n",
    "    f = lambda x, y: ((x - y) ** 2).sum(-1).mean()\n",
    "\n",
    "    _mse_pos = f(x.pos,  ref_x.pos)\n",
    "    _mse_rot = f(quaternion_to_rotation_6d(x.rot),\n",
    "                 quaternion_to_rotation_6d(ref_x.rot))\n",
    "    _mse_vel = f(xd.vel, ref_xd.vel)\n",
    "    _mse_ang = f(xd.ang, ref_xd.ang)\n",
    "\n",
    "    # ほぼ同じ大きさになるように調整\n",
    "    return _mse_pos      \\\n",
    "      + 0.1 * _mse_rot   \\\n",
    "      + 0.01 * _mse_vel  \\\n",
    "      + 0.001 * _mse_ang\n",
    "\n",
    "  def _reward_min_reference_tracking(self, ref_qpos, ref_qvel, state):\n",
    "    \"\"\" \n",
    "    最小座標を使用。関節角度の追跡精度を向上させます。\n",
    "    \"\"\"\n",
    "    pos = jp.concatenate([\n",
    "      state.pipeline_state.qpos[:3],\n",
    "      state.pipeline_state.qpos[7:]])\n",
    "    pos_targ = jp.concatenate([\n",
    "      ref_qpos[:3],\n",
    "      ref_qpos[7:]])\n",
    "    pos_err = jp.linalg.norm(pos_targ - pos)\n",
    "    vel_err = jp.linalg.norm(state.pipeline_state.qvel- ref_qvel)\n",
    "\n",
    "    return pos_err + vel_err\n",
    "\n",
    "  def _reward_feet_height(self, feet_pos, feet_pos_ref):\n",
    "    return jp.sum(jp.abs(feet_pos - feet_pos_ref)) # L1ノルムを使って0に近づけようとする\n",
    "\n",
    "envs.register_environment('trotting_anymal', TrotAnymal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FoPGによる模倣学習\n",
    "NVIDIA 3060 TI GPUで15分かかります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_networks_factory = functools.partial(\n",
    "    apg_networks.make_apg_networks,\n",
    "    hidden_layer_sizes=(256, 128)\n",
    ")\n",
    "\n",
    "epochs = 499\n",
    "\n",
    "train_fn = functools.partial(apg.train,\n",
    "                             episode_length=240,\n",
    "                             policy_updates=epochs,\n",
    "                             horizon_length=32,\n",
    "                             num_envs=64,\n",
    "                             learning_rate=1e-4,\n",
    "                             num_eval_envs=64,\n",
    "                             num_evals=10 + 1,\n",
    "                             use_float64=True,\n",
    "                             normalize_observations=True,\n",
    "                             network_factory=make_networks_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = []\n",
    "y_data = []\n",
    "ydataerr = []\n",
    "times = [datetime.now()]\n",
    "\n",
    "def progress(it, metrics):\n",
    "  times.append(datetime.now())\n",
    "  x_data.append(it)\n",
    "  y_data.append(metrics['eval/episode_reward'])\n",
    "  ydataerr.append(metrics['eval/episode_reward_std'])\n",
    "\n",
    "# 各足は毎秒2回地面に接触する\n",
    "env = envs.get_environment(\"trotting_anymal\", step_k = 13)\n",
    "eval_env = envs.get_environment(\"trotting_anymal\", step_k = 13)\n",
    "\n",
    "make_inference_fn, params, _= train_fn(environment=env,\n",
    "                                       progress_fn=progress,\n",
    "                                       eval_env=eval_env)\n",
    "\n",
    "plt.errorbar(x_data, y_data, yerr=ydataerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_env = envs.training.EpisodeWrapper(env, \n",
    "                                        episode_length=1000, \n",
    "                                        action_repeat=1)\n",
    "\n",
    "render_rollout(\n",
    "  jax.jit(demo_env.reset),\n",
    "  jax.jit(demo_env.step),\n",
    "  jax.jit(make_inference_fn(params)),\n",
    "  demo_env,\n",
    "  n_steps=200,\n",
    "  seed=1\n",
    ")\n",
    "\n",
    "model_path = '/tmp/trotting_2hz_policy'\n",
    "model.save_params(model_path, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**サンプル効率に関する注意**\n",
    "\n",
    "上記では、epochs * horizon_length * num_envs = 1.024e6のシミュレータステップを使用して学習しています。PPOで10倍のサンプルを使用した場合の結果を見てみましょう："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn = functools.partial(\n",
    "    ppo.train, num_timesteps=10_000_000, num_evals=10, reward_scaling=0.1,\n",
    "    episode_length=1000, normalize_observations=True, action_repeat=1,\n",
    "    unroll_length=10, num_minibatches=32, num_updates_per_batch=8,\n",
    "    discounting=0.97, learning_rate=3e-4, entropy_cost=1e-3, num_envs=1024,\n",
    "    batch_size=1024, seed=0)\n",
    "\n",
    "x_data = []\n",
    "y_data = []\n",
    "ydataerr = []\n",
    "env = envs.get_environment(\"trotting_anymal\", step_k = 13)\n",
    "\n",
    "def progress(num_steps, metrics):\n",
    "  x_data.append(num_steps)\n",
    "  y_data.append(metrics['eval/episode_reward'])\n",
    "  ydataerr.append(metrics['eval/episode_reward_std'])\n",
    "\n",
    "make_inference_fn, params, _= train_fn(environment=env, progress_fn=progress)\n",
    "\n",
    "plt.errorbar(x_data, y_data, yerr=ydataerr)\n",
    "plt.xlabel('# environment steps')\n",
    "plt.ylabel('reward per episode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPOがAPGに追いつくには約9e6のシミュレータステップが必要であることがわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## スタディ2：四足歩行ロボットの移動\n",
    "\n",
    "模倣学習の例で見たように、FoPG法は詳細な報酬信号の恩恵を受けます。移動を教えるために、Raibertヒューリスティックに基づいて足に報酬を与えます。[先行研究](https://arxiv.org/abs/2403.14864) と同様に、対角の脚ペアが固定周波数で同期して動くようにインセンティブを与える歩行スケジュールを使用します。新しいスケジュールされたステップの開始時に、ステップの終了時の足の目標位置を計算します。\n",
    "\n",
    "各足について、以下を計算します：\n",
    "\n",
    "$$\n",
    "p^* = h_0 + \\frac{\\Delta T}{2} v_0\n",
    "$$\n",
    "\n",
    "ここで $p^*$ は足の目標位置のx, y成分、$h_0$ は離地時の対応する股関節のx, y成分、$\\Delta T$ はスケジュールされたステップの持続時間、$v_0$ は離地時のベース速度です。\n",
    "\n",
    "探索能力が限られているため、FoPG法はポリシーの良い「初期推定」を持つことで大きな恩恵を受けます。これはモデル予測制御や軌道最適化ではよく知られた用語です。問題を [残差学習](https://arxiv.org/abs/1512.03385) として定式化します。$\\phi$ を既に持っているベースラインポリシーのパラメータとし、$f$ と $g$ をそれぞれ学習されるポリシーとベースラインポリシーのニューラルネットワークとします。$\\phi$ を固定し、以下のポリシーのパラメータ $\\theta$ を学習します：\n",
    "\n",
    "$$\n",
    "a_t = f(g(x_t; \\phi), x_t; \\theta) + g(x_t; \\phi)\n",
    "$$\n",
    "\n",
    "前のセクションのその場トロット歩行ポリシーを $\\phi$ として使用し、$x_t$ は時刻tの状態を表します。この例では0.75 m/sの速度目標を追跡しますが、より速いトロット歩行の方が安定するため、より速い速度目標に対して $\\phi$ を試すこともできます！\n",
    "\n",
    "単純にパラメータ $\\theta$ を $\\phi$ として初期化し、ポリシー $a_t = f(x_t; \\theta)$ で学習を「ホットスタート」する方がより自然に思えるかもしれませんが、実際には残差法の方がより安定して学習できることがわかっています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RL環境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def axis_angle_to_quaternion(v: jp.ndarray, theta:jp.float_):\n",
    "    \"\"\" \n",
    "    軸角度表現：vの周りにthetaだけ回転。\n",
    "    \"\"\"    \n",
    "    return jp.concatenate([jp.cos(0.5*theta).reshape(1), jp.sin(0.5*theta)*v.reshape(3)])\n",
    "\n",
    "def get_config():\n",
    "  \"\"\"anymal四足歩行ロボット環境の報酬設定を返します。\"\"\"\n",
    "\n",
    "  def get_default_rewards_config():\n",
    "    default_config = config_dict.ConfigDict(\n",
    "        dict(\n",
    "            scales=config_dict.ConfigDict(\n",
    "                dict(\n",
    "                    tracking_lin_vel = 1.0,\n",
    "                    orientation = -1.0, # 水平でないベース\n",
    "                    height = 0.5,\n",
    "                    lin_vel_z=-1.0, # 自殺ポリシーを防止\n",
    "                    torque = -0.01,\n",
    "                    feet_pos = -1, # 不良アクションのハードコーディング\n",
    "                    feet_height = -1, # 静止を防止\n",
    "                    joint_velocity = -0.001\n",
    "                    )\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    return default_config\n",
    "\n",
    "  default_config = config_dict.ConfigDict(\n",
    "      dict(rewards=get_default_rewards_config(),))\n",
    "\n",
    "  return default_config\n",
    "\n",
    "class FwdTrotAnymal(PipelineEnv):\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      termination_height: float=0.25,\n",
    "      **kwargs,\n",
    "  ):\n",
    "    \n",
    "    self.target_vel = kwargs.pop('target_vel', 0.75)\n",
    "    step_k = kwargs.pop('step_k', 25)\n",
    "    self.baseline_inference_fn = kwargs.pop(\"baseline_inference_fn\")\n",
    "    physics_steps_per_control_step = 10\n",
    "    kwargs['n_frames'] = kwargs.get(\n",
    "        'n_frames', physics_steps_per_control_step)\n",
    "    self.termination_height = termination_height\n",
    "\n",
    "    mj_model = mujoco.MjModel.from_xml_path(xml_path)\n",
    "    kp = 230\n",
    "    mj_model.actuator_gainprm[:, 0] = kp\n",
    "    mj_model.actuator_biasprm[:, 1] = -kp\n",
    "    self._init_q = mj_model.keyframe('standing').qpos\n",
    "    self._default_ap_pose = mj_model.keyframe('standing').qpos[7:]\n",
    "    self.reward_config = get_config()\n",
    "\n",
    "    self.action_loc = self._default_ap_pose\n",
    "    self.action_scale = jp.array([0.2, 0.8, 0.8] * 4)\n",
    "    \n",
    "    self.target_h = self._init_q[2]\n",
    "\n",
    "    sys = mjcf.load_model(mj_model)\n",
    "    super().__init__(sys=sys, **kwargs)\n",
    "    \n",
    "    \"\"\"\n",
    "    キネマティクス参照は歩行スケジューリングに使用されます。\n",
    "    \"\"\"\n",
    "\n",
    "    kinematic_ref_qpos = make_kinematic_ref(\n",
    "      cos_wave, step_k, scale=0.3, dt=self.dt)\n",
    "    self.l_cycle = jp.array(kinematic_ref_qpos.shape[0])\n",
    "    self.kinematic_ref_qpos = jp.array(kinematic_ref_qpos + self._default_ap_pose)\n",
    "\n",
    "    \"\"\"\n",
    "    足の追跡\n",
    "    \"\"\"\n",
    "    gait_k = step_k * 2\n",
    "    self.gait_period = gait_k * self.dt\n",
    "\n",
    "    self.step_k = step_k\n",
    "    self.feet_inds = jp.array([21,28,35,42]) # LF, RF, LH, RH\n",
    "    self.hip_inds = self.feet_inds - 6\n",
    "\n",
    "    self.pipeline_step = jax.checkpoint(self.pipeline_step,\n",
    "      policy=jax.checkpoint_policies.dots_with_no_batch_dims_saveable)\n",
    "    \n",
    "  def reset(self, rng: jax.Array) -> State:\n",
    "    rng, key_xyz, key_ang, key_ax, key_q, key_qd = jax.random.split(rng, 6)\n",
    "\n",
    "    qpos = jp.array(self._init_q)\n",
    "    qvel = jp.zeros(18)\n",
    "    \n",
    "    #### ランダム性の追加 ####\n",
    "  \n",
    "    r_xyz = 0.2 * (jax.random.uniform(key_xyz, (3,))-0.5)\n",
    "    r_angle = (jp.pi/12) * (jax.random.uniform(key_ang, (1,)) - 0.5) # 15度の範囲\n",
    "    r_axis = (jax.random.uniform(key_ax, (3,)) - 0.5)\n",
    "    r_axis = r_axis / jp.linalg.norm(r_axis)\n",
    "    r_quat = axis_angle_to_quaternion(r_axis, r_angle)\n",
    "\n",
    "    r_joint_q = 0.2 * (jax.random.uniform(key_q, (12,)) - 0.5)\n",
    "    r_joint_qd = 0.1 * (jax.random.uniform(key_qd, (12,)) - 0.5)\n",
    "  \n",
    "    qpos = qpos.at[0:3].set(qpos[0:3] + r_xyz)\n",
    "    qpos = qpos.at[3:7].set(r_quat)\n",
    "    qpos = qpos.at[7:19].set(qpos[7:19] + r_joint_q)\n",
    "    qvel = qvel.at[6:18].set(qvel[6:18] + r_joint_qd)\n",
    "    \n",
    "    data = self.pipeline_init(qpos, qvel)\n",
    "\n",
    "    # 地面にめり込まず、地面の上にいることを保証\n",
    "    pen = jp.min(data.contact.dist)\n",
    "    qpos = qpos.at[2].set(qpos[2] - pen)\n",
    "    data = self.pipeline_init(qpos, qvel)\n",
    "\n",
    "    state_info = {\n",
    "        'rng': rng,\n",
    "        'steps': 0.0,\n",
    "        'reward_tuple': {\n",
    "            'tracking_lin_vel': 0.0,\n",
    "            'orientation': 0.0,\n",
    "            'height': 0.0,\n",
    "            'lin_vel_z': 0.0,\n",
    "            'torque': 0.0,\n",
    "            'joint_velocity': 0.0,\n",
    "            'feet_pos': 0.0,\n",
    "            'feet_height': 0.0\n",
    "        },\n",
    "        'last_action': jp.zeros(12), # MJXチュートリアルより\n",
    "        'baseline_action': jp.zeros(12),\n",
    "        'xy0': jp.zeros((4, 2)),\n",
    "        'k0': 0.0,\n",
    "        'xy*': jp.zeros((4, 2))\n",
    "    }\n",
    "\n",
    "    x, xd = data.x, data.xd\n",
    "    _obs = self._get_obs(data.qpos, x, xd, state_info) # 内部観測（トロッターへ）\n",
    "  \n",
    "    action_key, key = jax.random.split(state_info['rng'])\n",
    "    state_info['rng'] = key\n",
    "    next_action, _ = self.baseline_inference_fn(_obs, action_key)\n",
    "\n",
    "    obs = jp.concatenate([_obs, next_action])\n",
    "\n",
    "    reward, done = jp.zeros(2)\n",
    "    metrics = {}\n",
    "    for k in state_info['reward_tuple']:\n",
    "      metrics[k] = state_info['reward_tuple'][k]\n",
    "    state = State(data, obs, reward, done, metrics, state_info)\n",
    "    return jax.lax.stop_gradient(state)\n",
    "\n",
    "  def step(self, state: State, action: jax.Array) -> State:\n",
    "\n",
    "    action = jp.clip(action, -1, 1)\n",
    "\n",
    "    cur_base = state.obs[-12:]\n",
    "    action += cur_base\n",
    "    state.info['baseline_action'] = cur_base\n",
    "\n",
    "    action = self.action_loc + (action * self.action_scale)\n",
    "\n",
    "    data = self.pipeline_step(state.pipeline_state, action)\n",
    "    \n",
    "    # 観測データ\n",
    "    x, xd = data.x, data.xd\n",
    "    obs = self._get_obs(data.qpos, x, xd, state.info)\n",
    "\n",
    "    # 転倒または落下した場合に終了\n",
    "    done = 0.0\n",
    "    done = jp.where(x.pos[0, 2] < self.termination_height, 1.0, done)\n",
    "    up = jp.array([0.0, 0.0, 1.0])\n",
    "    done = jp.where(jp.dot(math.rotate(up, x.rot[0]), up) < 0, 1.0, done)\n",
    "\n",
    "    #### 足の位置参照の更新 ####\n",
    "\n",
    "    # 新しいステップの開始を検出\n",
    "    s = state.info['steps']\n",
    "    step_num = s // (self.step_k)\n",
    "    even_step = step_num % 2 == 0\n",
    "    new_step = (s % self.step_k) == 0\n",
    "    new_even_step = jp.logical_and(new_step, even_step)\n",
    "    new_odd_step = jp.logical_and(new_step, jp.logical_not(even_step))\n",
    "\n",
    "    # Raibertヒューリスティックを適用して、ステップ後の足の目標位置を計算\n",
    "    hip_xy = data.geom_xpos[self.hip_inds][:,:2] # 4 x 2\n",
    "    v_body = data.qvel[0:2]\n",
    "    step_period = self.gait_period/2\n",
    "    raibert_xy = hip_xy + (step_period/2) * v_body\n",
    "\n",
    "    # 更新\n",
    "    cur_tars = state.info['xy*']\n",
    "    i_RFLH = jp.array([1, 2])\n",
    "    i_LFRH = jp.array([0, 3])\n",
    "    feet_xy = data.geom_xpos[self.feet_inds][:,:2]\n",
    "    \n",
    "    # トロット歩行では、対角の脚ペアの一方を動かし、\n",
    "    # もう一方を固定する\n",
    "    case_c1 = raibert_xy.at[i_LFRH].set(feet_xy[i_LFRH]) \n",
    "    case_c2 = raibert_xy.at[i_RFLH].set(feet_xy[i_RFLH])\n",
    "    xy_tars = jp.where(new_even_step, case_c1, cur_tars)\n",
    "    xy_tars = jp.where(new_odd_step, case_c2, xy_tars)\n",
    "    state.info['xy*'] = xy_tars\n",
    "\n",
    "    # ステップ開始時のタイムステップと位置を保存\n",
    "    state.info['k0'] = jp.where(new_step,\n",
    "                                state.info['steps'],\n",
    "                                state.info['k0'])\n",
    "    state.info['xy0'] = jp.where(new_step, \n",
    "                                 feet_xy,\n",
    "                                 state.info['xy0'])\n",
    "\n",
    "    # 報酬\n",
    "    reward_tuple = {\n",
    "        'tracking_lin_vel': (\n",
    "            self._reward_tracking_lin_vel(jp.array([self.target_vel, 0, 0]), x, xd)\n",
    "            * self.reward_config.rewards.scales.tracking_lin_vel\n",
    "        ),\n",
    "        'orientation': (\n",
    "          self._reward_orientation(x)\n",
    "          * self.reward_config.rewards.scales.orientation\n",
    "        ),\n",
    "        'lin_vel_z': (\n",
    "            self._reward_lin_vel_z(xd)\n",
    "            * self.reward_config.rewards.scales.lin_vel_z\n",
    "        ),\n",
    "        'height': (\n",
    "          self._reward_height(data.qpos) \n",
    "          * self.reward_config.rewards.scales.height\n",
    "        ),\n",
    "        'torque': (\n",
    "          self._reward_action(data.qfrc_actuator)\n",
    "          * self.reward_config.rewards.scales.torque\n",
    "        ),\n",
    "        'joint_velocity': (\n",
    "          self._reward_joint_velocity(data.qvel)\n",
    "          * self.reward_config.rewards.scales.joint_velocity\n",
    "        ),\n",
    "        'feet_pos': (\n",
    "          self._reward_feet_pos(data, state)\n",
    "          * self.reward_config.rewards.scales.feet_pos\n",
    "        ),\n",
    "        'feet_height': (\n",
    "          self._reward_feet_height(data, state.info)\n",
    "          * self.reward_config.rewards.scales.feet_height\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    reward = sum(reward_tuple.values())\n",
    "\n",
    "    # 状態管理\n",
    "    state.info['reward_tuple'] = reward_tuple\n",
    "    state.info['last_action'] = action\n",
    "\n",
    "    for k in state.info['reward_tuple'].keys():\n",
    "      state.metrics[k] = state.info['reward_tuple'][k]\n",
    "\n",
    "    # 次のアクション\n",
    "    action_key, key = jax.random.split(state.info['rng'])\n",
    "    state.info['rng'] = key\n",
    "    next_action, _ = self.baseline_inference_fn(obs, action_key)\n",
    "    obs = jp.concatenate([obs, next_action])\n",
    "\n",
    "    state = state.replace(\n",
    "        pipeline_state=data, obs=obs, reward=reward,\n",
    "        done=done)\n",
    "    return state\n",
    "\n",
    "  def _get_obs(self, qpos: jax.Array, x: Transform, xd: Motion,\n",
    "               state_info: Dict[str, Any]) -> jax.Array:\n",
    "\n",
    "    inv_base_orientation = math.quat_inv(x.rot[0])\n",
    "    local_rpyrate = math.rotate(xd.ang[0], inv_base_orientation)\n",
    "\n",
    "    obs_list = []\n",
    "    # ヨーレート\n",
    "    obs_list.append(jp.array([local_rpyrate[2]]) * 0.25)\n",
    "    # 射影重力\n",
    "    obs_list.append(\n",
    "        math.rotate(jp.array([0.0, 0.0, -1.0]), inv_base_orientation))\n",
    "    # モーター角度\n",
    "    angles = qpos[7:19]\n",
    "    obs_list.append(angles - self._default_ap_pose)\n",
    "    # 前回のアクション\n",
    "    obs_list.append(state_info['last_action'])\n",
    "    # 歩行スケジュール\n",
    "    kin_ref = self.kinematic_ref_qpos[jp.array(state_info['steps']%self.l_cycle, int)]\n",
    "    obs_list.append(kin_ref)\n",
    "\n",
    "    obs = jp.clip(jp.concatenate(obs_list), -100.0, 100.0)\n",
    "\n",
    "    return obs\n",
    "\n",
    "  # ------------ 報酬関数 ----------------\n",
    "  def _reward_tracking_lin_vel(\n",
    "      self, commands: jax.Array, x: Transform, xd: Motion) -> jax.Array:\n",
    "    # 線形速度コマンドの追従（xy軸）\n",
    "    local_vel = math.rotate(xd.vel[0], math.quat_inv(x.rot[0]))\n",
    "    lin_vel_error = jp.sum(jp.square(commands[:2] - local_vel[:2]))\n",
    "    lin_vel_reward = jp.exp(-lin_vel_error)\n",
    "    return lin_vel_reward\n",
    "  def _reward_orientation(self, x: Transform) -> jax.Array:\n",
    "    # 水平でないベース姿勢にペナルティ\n",
    "    up = jp.array([0.0, 0.0, 1.0])\n",
    "    rot_up = math.rotate(up, x.rot[0])\n",
    "    return jp.sum(jp.square(rot_up[:2]))\n",
    "  def _reward_lin_vel_z(self, xd: Motion) -> jax.Array:\n",
    "    # z軸のベース線形速度にペナルティ\n",
    "    return jp.clip(jp.square(xd.vel[0, 2]), 0, 10)\n",
    "  def _reward_joint_velocity(self, qvel):\n",
    "      return jp.clip(jp.sqrt(jp.sum(jp.square(qvel[6:]))), 0, 100)\n",
    "  def _reward_height(self, qpos) -> jax.Array:\n",
    "    return jp.exp(-jp.abs(qpos[2] - self.target_h)) # 1メートル以上の高さにはならない\n",
    "  def _reward_action(self, action) -> jax.Array:\n",
    "    return jp.sqrt(jp.sum(jp.square(action)))\n",
    "  def _reward_feet_pos(self, data, state):        \n",
    "    dt = (state.info['steps'] - state.info['k0']) * self.dt # スカラー\n",
    "    step_period = self.gait_period / 2\n",
    "    xyt = state.info['xy0'] + (state.info['xy*'] - state.info['xy0']) * (dt/step_period)\n",
    "\n",
    "    feet_pos = data.geom_xpos[self.feet_inds][:, :2]\n",
    "\n",
    "    rews = jp.sum(jp.square(feet_pos - xyt), axis=1)   \n",
    "    rews = jp.clip(rews, 0, 10)\n",
    "    return jp.sum(rews)\n",
    "  def _reward_feet_height(self, data, state_info):\n",
    "    \"\"\" \n",
    "    足の高さが整流正弦波を追跡する\n",
    "    \"\"\"\n",
    "    h_tar = 0.1\n",
    "    t = state_info['steps'] * self.dt\n",
    "    offset = self.gait_period/2\n",
    "    ref1 = jp.sin((2*jp.pi/self.gait_period)*t) # RFとLHの足\n",
    "    ref2 = jp.sin((2*jp.pi/self.gait_period)*(t - offset)) # LFとRHの足\n",
    "    \n",
    "    ref1, ref2 = ref1 * h_tar, ref2 * h_tar\n",
    "    h_tars = jp.array([ref2, ref1, ref1, ref2])\n",
    "    h_tars = h_tars.clip(min=0, max=None) + 0.02 # 足の高さオフセット\n",
    "    \n",
    "    feet_height = data.geom_xpos[self.feet_inds][:,2]\n",
    "    errs = jp.clip(jp.square(feet_height - h_tars), 0, 10)\n",
    "    return jp.sum(errs)\n",
    "  \n",
    "envs.register_environment('anymal', FwdTrotAnymal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FoPGによる残差学習\n",
    "NVIDIA 3060 TI GPUで15分かかります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トロット歩行の推論関数を再構築\n",
    "make_networks_factory = functools.partial(\n",
    "    apg_networks.make_apg_networks,\n",
    "    hidden_layer_sizes=(256, 128)\n",
    ")\n",
    "\n",
    "nets = make_networks_factory(observation_size=1, # observation_size引数はパラメータ初期化にのみ使用されるため重要ではない\n",
    "                             action_size=12,\n",
    "                             preprocess_observations_fn=running_statistics.normalize)\n",
    "\n",
    "make_inference_fn = apg_networks.make_inference_fn(nets)\n",
    "\n",
    "# 移動学習の設定\n",
    "make_networks_factory = functools.partial(\n",
    "    apg_networks.make_apg_networks,\n",
    "    hidden_layer_sizes=(128, 64)\n",
    ")\n",
    "\n",
    "epochs = 499\n",
    "\n",
    "train_fn = functools.partial(apg.train,\n",
    "                             episode_length=1000,\n",
    "                             policy_updates=epochs,\n",
    "                             horizon_length=32,\n",
    "                             num_envs=64,\n",
    "                             learning_rate=1.5e-4,\n",
    "                             schedule_decay=0.995,\n",
    "                             num_eval_envs=64,\n",
    "                             num_evals=10 + 1,\n",
    "                             use_float64=True,\n",
    "                             normalize_observations=True,\n",
    "                             network_factory=make_networks_factory)\n",
    "\n",
    "model_path = '/tmp/trotting_2hz_policy'\n",
    "params = model.load_params(model_path)\n",
    "baseline_inference_fn = make_inference_fn(params)\n",
    "\n",
    "env_kwargs = dict(target_vel=0.75, step_k=13, \n",
    "                  baseline_inference_fn=baseline_inference_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = []\n",
    "y_data = []\n",
    "ydataerr = []\n",
    "times = [datetime.now()]\n",
    "\n",
    "def progress(it, metrics):\n",
    "  times.append(datetime.now())\n",
    "  x_data.append(it)\n",
    "  y_data.append(metrics['eval/episode_reward'])\n",
    "  ydataerr.append(metrics['eval/episode_reward_std'])\n",
    "\n",
    "env = envs.get_environment(\"anymal\", **env_kwargs)\n",
    "eval_env = envs.get_environment(\"anymal\", **env_kwargs)\n",
    "\n",
    "make_inference_fn, params, _= train_fn(environment=env,\n",
    "                                       progress_fn=progress,\n",
    "                                       eval_env=eval_env)\n",
    "\n",
    "plt.errorbar(x_data, y_data, yerr=ydataerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_env = envs.training.EpisodeWrapper(env, \n",
    "                                        episode_length=1000, \n",
    "                                        action_repeat=1)\n",
    "\n",
    "render_rollout(\n",
    "  jax.jit(demo_env.reset),\n",
    "  jax.jit(demo_env.step),\n",
    "  jax.jit(make_inference_fn(params)),\n",
    "  demo_env,\n",
    "  n_steps=200,\n",
    "  camera=\"track\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**サンプル効率に関する注意**\n",
    "\n",
    "PPOで再度1e7サンプルを使用した場合と比較してみましょう："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn = functools.partial(\n",
    "    ppo.train, num_timesteps=10_000_000, num_evals=10, reward_scaling=0.1,\n",
    "    episode_length=1000, normalize_observations=True, action_repeat=1,\n",
    "    unroll_length=10, num_minibatches=32, num_updates_per_batch=8,\n",
    "    discounting=0.97, learning_rate=3e-4, entropy_cost=1e-3, num_envs=1024,\n",
    "    batch_size=1024, seed=0)\n",
    "\n",
    "x_data = []\n",
    "y_data = []\n",
    "ydataerr = []\n",
    "\n",
    "env = envs.get_environment(\"anymal\", **env_kwargs)\n",
    "\n",
    "def progress(num_steps, metrics):\n",
    "  x_data.append(num_steps)\n",
    "  y_data.append(metrics['eval/episode_reward'])\n",
    "  ydataerr.append(metrics['eval/episode_reward_std'])\n",
    "\n",
    "make_inference_fn, params, _= train_fn(environment=env, progress_fn=progress)\n",
    "\n",
    "plt.errorbar(x_data, y_data, yerr=ydataerr)\n",
    "plt.xlabel('# environment steps')\n",
    "plt.ylabel('reward per episode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPOはこの設定では移動の学習に苦戦し、10倍以上のシミュレータステップを使用しても難しいことがわかります。\n",
    "\n",
    "これはPPOの欠点を示しているのではなく、FoPG法を効果的に活用するポリシー学習のセットアップを実証しています。良いベースラインからの小さく正確な摂動を学習すること、つまり深い谷のローカルミニマムに最適化することが含まれます。FoPGは、足の配置スプラインのような微妙な報酬信号を使ってこれらの摂動を導くのに十分な精度を持っています。\n",
    "\n",
    "一方、PPOなどのRLアルゴリズムは、より構造化されていない報酬を持つ [ポリシー学習のセットアップ](https://colab.research.google.com/github/google-deepmind/mujoco/blob/main/mjx/tutorial.ipynb) で恩恵を受けます。FoPG法とは異なり、転倒時の大きなペナルティなどのスパースで非微分可能な報酬から [大きな恩恵](https://www.science.org/doi/abs/10.1126/scirobotics.adg1462) を受けます。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}